{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "JcW4OyDwgcE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies (once)\n",
        "!pip install transformers  # maybe also datasets, torchvision, etc.\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgjonoyNIz3r",
        "outputId": "6c8319e8-ac47-49a0-fbc4-fdf7895be99b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ud2kuc5g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ud2kuc5g\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=49a697fa5da442d90ca1ab751e8c131d274206df2d9808be40a12ea6d7f1c31a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qr4l25_9/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "szO12SBuYfE6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor, AutoModel\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import clip\n",
        "\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "kfSHyhoqUen0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def unnormalize(x):\n",
        "    \"\"\"\n",
        "    x: tensor of shape (B,3,H,W) or (3,H,W), CLIP-normalized.\n",
        "    Returns: unnormalized image in [0,1].\n",
        "    \"\"\"\n",
        "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073],\n",
        "                        device=x.device)[None, :, None, None]\n",
        "    std  = torch.tensor([0.26862954, 0.26130258, 0.27577711],\n",
        "                        device=x.device)[None, :, None, None]\n",
        "\n",
        "    # If x is (3,H,W), add batch dimension\n",
        "    if x.dim() == 3:\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "    x_unnorm = x * std + mean\n",
        "    return x_unnorm.clamp(0, 1)\n",
        "\n",
        "def layer_norm(x):\n",
        "    ln = nn.LayerNorm(768, elementwise_affine=False).to(device)\n",
        "    return ln(x)\n",
        "\n",
        "def print_stats(x):\n",
        "    print(\"mean:\", x.mean().item())\n",
        "    print(\"std:\", x.std().item())\n",
        "    print(\"min:\", x.min().item())\n",
        "    print(\"max:\", x.max().item())\n"
      ],
      "metadata": {
        "id": "m5abixHLUgZp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "tLoBOwmhgjFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sparse Autoencoder"
      ],
      "metadata": {
        "id": "gXeMQ3uwVG6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Model Architecture (taken from HF)\n",
        "Input Dimension: 768\n",
        "SAE Dimension: 49,152\n",
        "Expansion Factor: x64 (vanilla architecture)\n",
        "Activation Function: ReLU\n",
        "Initialization: encoder_transpose_decoder\n",
        "Context Size: 50 tokens\n",
        "'''\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=49152):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.encoder = nn.Parameter(torch.zeros(input_dim, hidden_dim))\n",
        "        self.decoder = nn.Parameter(torch.zeros(hidden_dim, input_dim))\n",
        "\n",
        "        self.b_enc = nn.Parameter(torch.zeros(hidden_dim))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(input_dim))\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.forward(x)[\"acts\"]\n",
        "\n",
        "    def decode(self, acts):\n",
        "        recon = acts @ self.decoder + self.b_dec\n",
        "        return recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = x @ self.encoder + self.b_enc\n",
        "        acts = F.relu(encoded)\n",
        "\n",
        "        # Decode using tied weights\n",
        "        recon = acts @ self.decoder + self.b_dec\n",
        "\n",
        "        return {\n",
        "            \"acts\": acts,\n",
        "            \"reconstruction\": recon\n",
        "        }\n"
      ],
      "metadata": {
        "id": "aBJ79JeRCpo7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sae_from_pt(path):\n",
        "    raw_state = torch.load(path, map_location=\"cpu\")\n",
        "\n",
        "    # Infer sizes\n",
        "    input_dim, hidden_dim = raw_state[\"W_enc\"].shape\n",
        "    sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
        "\n",
        "    # Adapt keys to match your model's expected names\n",
        "    state = {\n",
        "        \"encoder\": raw_state[\"W_enc\"],\n",
        "        \"decoder\": raw_state[\"W_dec\"],\n",
        "        \"b_enc\":   raw_state[\"b_enc\"],\n",
        "        \"b_dec\":   raw_state[\"b_dec\"],\n",
        "    }\n",
        "\n",
        "    sae.load_state_dict(state)\n",
        "    return sae"
      ],
      "metadata": {
        "id": "t7SRgIrcFcgb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_sae_from_pt(\"weights.pt\").to(device)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6AbxetrFfT8",
        "outputId": "87607e30-eeb6-461d-ac35-c68facbc7818"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseAutoencoder()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = torch.load(\"weights.pt\", map_location=\"cpu\")\n",
        "\n",
        "\n",
        "w = state[\"W_enc\"]\n",
        "print(\"mean:\", w.mean().item())\n",
        "print(\"std:\", w.std().item())\n",
        "print(\"min:\", w.min().item())\n",
        "print(\"max:\", w.max().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP7bdG6ZBHKb",
        "outputId": "6b7d9032-216a-4beb-ea0f-fe81fc8b18d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: -5.68432587897405e-05\n",
            "std: 0.02647518552839756\n",
            "min: -0.6612777709960938\n",
            "max: 0.7138082385063171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLIP Classifier"
      ],
      "metadata": {
        "id": "NNYP4p97VKal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "class CLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model_name=\"ViT-B/32\", num_classes=10, device=device):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load CLIP\n",
        "        self.clip_model, _ = clip.load(clip_model_name, device=device)\n",
        "        self.clip_model = self.clip_model.float()\n",
        "\n",
        "        # freeze CLIP parameters\n",
        "        for p in self.clip_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # create classification head\n",
        "        embed_dim = self.clip_model.visual.output_dim  # 512 for ViT-B/32\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # extract CLIP image features\n",
        "        feats = self.clip_model.encode_image(images)\n",
        "\n",
        "        # classifier\n",
        "        logits = self.classifier(feats)\n",
        "        return logits\n",
        "\n",
        "    def classify(self, feats):\n",
        "        return self.classifier(feats)\n",
        "\n",
        "    def ln(self, x):\n",
        "        return self.clip_model.visual.ln_post(x)\n",
        "\n",
        "    def precompute(self, x):\n",
        "        return self.clip_model.encode_image(x)\n",
        "\n",
        "    def head(self, images, split_at=11):\n",
        "        V = self.clip_model.visual\n",
        "\n",
        "        # Patch embedding\n",
        "        x = V.conv1(images)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # [B, N, C]\n",
        "\n",
        "        # CLS token\n",
        "        cls = V.class_embedding.to(x.dtype)\n",
        "        cls = cls.unsqueeze(0).unsqueeze(1).expand(x.shape[0], -1, -1)  # [B,1,C]\n",
        "        x = torch.cat([cls, x], dim=1)  # [B, 1+N, C]\n",
        "\n",
        "        # Positional embedding + pre-LN\n",
        "        x = x + V.positional_embedding.to(x.dtype)\n",
        "        x = V.ln_pre(x)\n",
        "\n",
        "        # Switch to CLIP transformer format: [L, B, C]\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        # Run layers 0..split_at\n",
        "        for i in range(split_at + 1):\n",
        "            x = V.transformer.resblocks[i](x)\n",
        "\n",
        "        # Convert back to batch-first for user-facing return\n",
        "        x = x.permute(1, 0, 2)  # [B, L, C]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def tail(self, x, split_at=11):\n",
        "        V = self.clip_model.visual\n",
        "\n",
        "        # x arrives as batch-first: [B, L, C]\n",
        "        # Convert to transformer format\n",
        "        x = x.permute(1, 0, 2)  # [L, B, C]\n",
        "\n",
        "        # Run remaining layers\n",
        "        for i in range(split_at + 1, len(V.transformer.resblocks)):\n",
        "            x = V.transformer.resblocks[i](x)\n",
        "\n",
        "        # Back to batch-first\n",
        "        x = x.permute(1, 0, 2)  # [B, L, C]\n",
        "\n",
        "        # Final post-LN should operate on the sequence\n",
        "        x = V.ln_post(x)\n",
        "\n",
        "        # CLS token\n",
        "        cls = x[:, 0, :]\n",
        "\n",
        "        # Projection\n",
        "        if hasattr(V, \"proj\") and V.proj is not None:\n",
        "            if isinstance(V.proj, torch.nn.Parameter):\n",
        "                cls = cls @ V.proj\n",
        "            else:\n",
        "                cls = V.proj(cls)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PoZSwtnbd8Fr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_classifier = CLIPClassifier().to(device)\n",
        "clip_classifier.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQVszItVg3EU",
        "outputId": "d4452e18-1fb3-438f-d235-ecf6f8afaf97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 60.8MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPClassifier(\n",
              "  (clip_model): CLIP(\n",
              "    (visual): VisionTransformer(\n",
              "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (transformer): Transformer(\n",
              "        (resblocks): Sequential(\n",
              "          (0): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (token_embedding): Embedding(49408, 512)\n",
              "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_classifier.classifier.load_state_dict(torch.load(\"classifier_weights.pt\"), strict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf0nUHx6i3B6",
        "outputId": "53459d43-3fcc-4059-cfdb-7df71fc95b1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "N7li_YSIglY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load cifar-10\n",
        "\n",
        "def get_cifar10_loaders(batch_size=128, num_workers=0, val_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Returns train, val, and test DataLoaders for CIFAR-10.\n",
        "    Train split is further split into train + val.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # CLIP expects 224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.48145466, 0.4578275, 0.40821073),\n",
        "            (0.26862954, 0.26130258, 0.27577711)\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    torch.manual_seed(67)\n",
        "\n",
        "    # Load the full 50k CIFAR-10 training set\n",
        "    full_train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform,)\n",
        "\n",
        "    # Split indices\n",
        "    val_size = int(len(full_train_dataset) * val_ratio)  # 5000 if val_ratio=0.1\n",
        "    train_size = len(full_train_dataset) - val_size      # 45000\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
        ")\n",
        "\n",
        "    # Test dataset (10k)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True,transform=transform)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers,pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset,batch_size=batch_size, shuffle=False,num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, test_dataset\n",
        "\n",
        "train_loader, val_loader, test_loader, test_dataset = get_cifar10_loaders()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npYNw-4felwY",
        "outputId": "84932ee8-3128-403e-8d1b-7c3f495fb319"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train classifier"
      ],
      "metadata": {
        "id": "JRh_Mn7Lgobw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP Embeddings (for training classification head)"
      ],
      "metadata": {
        "id": "uJaeXbsMLola"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# precompute train embeddings\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Precomputing CLIP embeddings...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        feats = clip_classifier.precompute(images)  # shape [batch, 512]\n",
        "\n",
        "        all_feats.append(feats.cpu())\n",
        "        all_labels.append(labels)\n",
        "\n",
        "all_feats = torch.cat(all_feats)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "print(\"Feature tensor shape:\", all_feats.shape)\n",
        "\n",
        "# dataset of precomputed embeddings\n",
        "feature_dataset = TensorDataset(all_feats, all_labels)\n",
        "feature_loader = DataLoader(feature_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTPfz2hFMi0A",
        "outputId": "206adf18-f14c-4323-917b-f1bc47a1afe7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precomputing CLIP embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [01:50<00:00,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature tensor shape: torch.Size([45000, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# precompute test embeddings\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Test CLIP embeddings...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        feats = clip_classifier.precompute(images)  # shape [batch, 512]\n",
        "\n",
        "        all_feats.append(feats.cpu())\n",
        "        all_labels.append(labels)\n",
        "\n",
        "all_feats = torch.cat(all_feats)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "print(\"Feature tensor shape:\", all_feats.shape)\n",
        "\n",
        "# dataset of precomputed embeddings\n",
        "test_feature_dataset = TensorDataset(all_feats, all_labels)\n",
        "test_feature_loader = DataLoader(test_feature_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AkVCG1lOuxo",
        "outputId": "fbc34ca4-5ec2-4036-ae43-137f908131f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test CLIP embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:24<00:00,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature tensor shape: torch.Size([10000, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "11G7BZTi-CVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "optimizer = optim.Adam(clip_classifier.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 20\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    clip_classifier.train()\n",
        "    avg_loss = 0\n",
        "\n",
        "    for images, labels in tqdm(feature_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        logits = clip_classifier.classify(images)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss/images.shape[0]:.4f}\")"
      ],
      "metadata": {
        "id": "LrNjkVr_gVzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "\n",
        "clip_classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_feature_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = clip_classifier.classify(images)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(\"Accuracy:\", correct / total)\n",
        "\n",
        "torch.save(clip_classifier.classifier.state_dict(), \"classifier_weights.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faBnZzwYd8JU",
        "outputId": "bc8bc9d3-cc03-4b23-ceac-55f0c736376f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projected Gradient Descent"
      ],
      "metadata": {
        "id": "NF_mXQLkiKTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pgd\n",
        "\n",
        "def PGD(\n",
        "    h,                      # [B, T, D] clean tokens\n",
        "    y,                      # labels\n",
        "    model_tail,             # tail(z) -> logits\n",
        "    loss_fn=nn.CrossEntropyLoss(),\n",
        "    scale=False,\n",
        "    eps=0.1,\n",
        "    alpha=1e-3,\n",
        "    num_steps=40,\n",
        "):\n",
        "    \"\"\"\n",
        "    PGD attack in CLIP token space.\n",
        "    h must be [B, T, D] and output will match exactly.\n",
        "    \"\"\"\n",
        "\n",
        "    # detach ensures h is a constant baseline, but keep shape\n",
        "    h_orig = h.detach()\n",
        "\n",
        "    if scale:\n",
        "        scale_hp = h_orig.pow(2).mean().sqrt() # rms norm over ALL tokens\n",
        "        eps = eps * scale_hp\n",
        "        alpha = eps * scale_hp\n",
        "\n",
        "    # initialize perturbation with correct shape\n",
        "    delta = torch.zeros_like(h_orig).uniform_(-eps, eps).to(device)\n",
        "\n",
        "    logits = model_tail(h_orig)\n",
        "    l0 = loss_fn(logits, y).detach().cpu().item()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        delta.requires_grad_(True)\n",
        "\n",
        "        h_adv = h_orig + delta          # preserve shape [B, T, D]\n",
        "        logits = model_tail(h_adv)\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        # compute gradient w.r.t delta only\n",
        "        (grad,) = torch.autograd.grad(loss, delta)\n",
        "\n",
        "        # PGD update in L∞\n",
        "        delta = delta + alpha * grad.sign()\n",
        "\n",
        "        # project back into L∞ ball — preserves shape\n",
        "        delta = torch.clamp(delta, -eps, eps)\n",
        "\n",
        "        # detach to prevent graph buildup\n",
        "        delta = delta.detach()\n",
        "\n",
        "    logits = model_tail(h_adv)\n",
        "    l_final = loss_fn(logits, y).detach().cpu().item()\n",
        "    loss_diff = l_final-l0\n",
        "\n",
        "    # print(f\"Loss difference:{loss_diff} \\n\")\n",
        "    # final adv tokens (same shape as h_orig)\n",
        "    return (h_orig + delta).detach(), loss_diff"
      ],
      "metadata": {
        "id": "X1n5TgNq5j5t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get activations and features"
      ],
      "metadata": {
        "id": "_WyoSuo97BPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input: x (image)\n",
        "\n",
        "Output:\n",
        "- h & SAE(h) : base\n",
        "- PGD(h) & SAE(h_adv) : function 1 aka which features set the model off the most\n",
        "- h(x_adv) & SAE(h(x_adv)) : function 2 aka which features get set off by perturbations the most\n",
        "'''\n",
        "\n",
        "def function_1(x, y, model, clip_classifier, alpha=1e-2, print_stats=False):\n",
        "    h = clip_classifier.head(x)\n",
        "    h_features = model.encode(layer_norm(h))\n",
        "    h_adv, loss_diff = PGD(h, y, clip_classifier.tail, scale=True)\n",
        "    h_adv_features = model.encode(layer_norm(h_adv))\n",
        "\n",
        "    if print_stats:\n",
        "        x = layer_norm(h_adv)\n",
        "        ('Activation shape: ' + h.shape)\n",
        "        print(\"Distribution before SAE\")\n",
        "        print_stats(x)\n",
        "\n",
        "    return {'h': h.detach().cpu(),\n",
        "            'h_features': h_features.detach().cpu(),\n",
        "            'h_adv':h_adv.detach().cpu(),\n",
        "            'h_adv_features': h_adv_features.detach().cpu(),\n",
        "            'loss_diff': loss_diff}\n",
        "\n",
        "def function_2(x, y, model, clip_classifier, alpha = 1e-2, print_stats=False):\n",
        "    x_adv, loss_diff = PGD(x, y, clip_classifier)\n",
        "    hx_adv = clip_classifier.head(x_adv)\n",
        "    hx_adv_features = model.encode(layer_norm(hx_adv))\n",
        "\n",
        "\n",
        "    if print_stats:\n",
        "        x = layer_norm(hx_adv)\n",
        "        print('Activation shape: ' + hx_adv.shape)\n",
        "        print(\"Distribution before SAE\")\n",
        "        print_stats(x)\n",
        "\n",
        "    return {'x_adv': x_adv.detach().cpu(),\n",
        "            'hx_adv':hx_adv.detach().cpu(),\n",
        "            'hx_adv_features': hx_adv_features.detach().cpu(),\n",
        "            'loss_diff': loss_diff}\n",
        "\n",
        "# non-functional\n",
        "def get_all_outputs(x, y, model, clip_classifier):\n",
        "    dict_1 = function_1(x, y, model, clip_classifier)\n",
        "    dict_2 = function_2(x, y, model, clip_classifier)\n",
        "    return {**dict_1, **dict_2}"
      ],
      "metadata": {
        "id": "rVKANzSb7PZY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data collection (2hr)"
      ],
      "metadata": {
        "id": "sRF32jxuzEIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests"
      ],
      "metadata": {
        "id": "J15A4eFUzBWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_iter = iter(test_loader)"
      ],
      "metadata": {
        "id": "gYD3FSuPdFj6"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label, idx = next(test_iter)\n",
        "\n",
        "image, label, idx = image[0].unsqueeze(0).to(device), label[0].unsqueeze(0).to(device), idx"
      ],
      "metadata": {
        "id": "5OB7KBXEBlIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = function_1(image, label, model, clip_classifier)\n",
        "sample_2 = function_2(image, label, model, clip_classifier)\n",
        "\n",
        "print(sample['h_adv_features'].shape)\n",
        "print(sample_2['hx_adv_features'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhL9C3r1VuYt",
        "outputId": "dae78326-92c0-41f7-b0e8-60dbf7fe76c9"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 49152])\n",
            "torch.Size([1, 50, 49152])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "PNjvxt60HRyU",
        "outputId": "8ae79278-a998-415c-d9e2-7909ea61c849"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3529508405.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_feature_fires(\n",
        "    class_loaders,\n",
        "    model,\n",
        "    clip_classifier,\n",
        "    activation_type: str,\n",
        "    function=function_2,\n",
        "    threshold=0.1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        class_feature_counts: dict[label -> (num_features,) tensor]\n",
        "    \"\"\"\n",
        "\n",
        "    # Detect number of features from model\n",
        "    num_features = model.hidden_dim\n",
        "\n",
        "    # Dictionary: class_label → feature firing counts\n",
        "    class_feature_counts = {}\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    save_dir = f\"/content/drive/MyDrive/deep_learning/text_outputs_{activation_type}_{threshold}_{timestamp}\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Loop over all classes\n",
        "    for label, loader in class_loaders.items():\n",
        "        print(f\"\\n===== Evaluating Class {label} =====\")\n",
        "\n",
        "        # Initialize class-specific counter\n",
        "        feature_counts = torch.zeros(num_features, dtype=torch.long)\n",
        "        loss_diff_list = []\n",
        "\n",
        "        # Loop over batches\n",
        "        for x_batch, y_batch in loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            out = function(x_batch, y_batch, model, clip_classifier)\n",
        "\n",
        "            loss_diff_list.append(out['loss_diff'])\n",
        "            feats = out[activation_type]       # (B, tokens, features)\n",
        "            binary_feats = (feats > threshold).sum(dim=(0,1))\n",
        "\n",
        "            feature_counts += binary_feats.cpu()\n",
        "\n",
        "            # free GPU memory\n",
        "            del x_batch, y_batch, feats, binary_feats, out\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Save per-class results\n",
        "        class_feature_counts[label] = feature_counts\n",
        "\n",
        "\n",
        "        # ---- Print top 10 features for that class ----\n",
        "        top_vals, top_idx = torch.topk(feature_counts, 50)\n",
        "\n",
        "        buf = io.StringIO()\n",
        "        buf2 = io.StringIO()\n",
        "\n",
        "        with redirect_stdout(buf):\n",
        "            print(f\"Top 50 firing features for class {label} [rank, feature, count]:\")\n",
        "            for rank in range(50):\n",
        "                print(f\"  [{rank+1}, {top_idx[rank].item()}, {top_vals[rank].item()}]\")\n",
        "\n",
        "        with redirect_stdout(buf2):\n",
        "            print(f\"Loss difference from perturbations for class {label} + \\n\")\n",
        "            for loss in loss_diff_list:\n",
        "                print(str(loss) + '\\n')\n",
        "\n",
        "        text_output = buf.getvalue() # contain the prints yurr\n",
        "        loss_output = buf2.getvalue()\n",
        "\n",
        "        filepath = os.path.join(save_dir, f\"top_50_{activation_type}_class_{label}.txt\")\n",
        "        filepath_loss = os.path.join(save_dir, f\"loss_diff_values_class_{label}.txt\")\n",
        "\n",
        "        with open(filepath, \"w\") as f:\n",
        "            f.write(text_output)\n",
        "        with open(filepath_loss, \"w\") as f:\n",
        "            f.write(loss_output)\n",
        "\n",
        "    zip_path = shutil.make_archive(f\"{save_dir}_zip\", \"zip\", save_dir)\n",
        "    print(\"Saved ZIP to:\", zip_path)\n",
        "\n",
        "    return class_feature_counts"
      ],
      "metadata": {
        "id": "cHmrUGrU2YCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_class_loaders(class_images, batch_size=100):\n",
        "    class_loaders = {}\n",
        "\n",
        "    for label, imgs in class_images.items():\n",
        "        imgs_tensor = torch.stack(imgs)   # (N, C, H, W)\n",
        "        ds = TensorDataset(imgs_tensor, torch.full((len(imgs),), label))\n",
        "        class_loaders[label] = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return class_loaders"
      ],
      "metadata": {
        "id": "tzGWCsdn0Sf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort test_dataset by class\n",
        "from collections import defaultdict\n",
        "\n",
        "class_images = defaultdict(list)\n",
        "\n",
        "for img, label in test_dataset:\n",
        "    class_images[label].append(img)"
      ],
      "metadata": {
        "id": "GNuHRcH-zGlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_loaders = build_class_loaders(class_images, batch_size=100)"
      ],
      "metadata": {
        "id": "jj6Tt3Am4ll8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# varying threshold\n",
        "\n",
        "thresholds = torch.linspace(start=1, end=3, steps=5).tolist()\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "my_path = f\"/content/drive/MyDrive/deep_learning/feature_counts_{timestamp}\"\n",
        "os.makedirs(my_path, exist_ok=True)\n",
        "\n",
        "activation_types = ['hx_adv_features', 'h_features', 'h_adv_features']# 2 x 5 x 24 min = 2 hrs\n",
        "for threshold in thresholds: # 5 x 24 min\n",
        "    for activation_type in activation_types: # 3 activations\n",
        "        if activation_type == 'hx_adv_features':\n",
        "            function = function_2\n",
        "        else:\n",
        "            function = function_1\n",
        "\n",
        "        feature_counts = count_feature_fires( # approx 24 mins\n",
        "        class_loaders=class_loaders,\n",
        "        model=model,\n",
        "        clip_classifier=clip_classifier,\n",
        "        activation_type=activation_type,\n",
        "        function=function,\n",
        "        threshold=threshold)\n",
        "\n",
        "        threshold_str = f\"{threshold:.2f}\"\n",
        "\n",
        "\n",
        "        torch.save(feature_counts, f\"{my_path}/{activation_type}_counts_{threshold_str}.pt\")\n"
      ],
      "metadata": {
        "id": "Ic12_5TaAtfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if (sample_2['hx_adv'] != 0).any():\n",
        "    print(\"Tensor has at least one non-zero value.\")\n",
        "else:\n",
        "    print(\"Tensor is all zeros.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-t2BvzqAyB6",
        "outputId": "b2305137-7361-4bf5-c74f-873e96850fb0"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor has at least one non-zero value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune the alphas"
      ],
      "metadata": {
        "id": "8HzPCgbhtUow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alpha tuning\n",
        "\n",
        "alphas = np.linspace(0.5, 1e-3, num=10).tolist()\n",
        "\n",
        "loss_1_diffs = []\n",
        "loss_2_diffs = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    sample = function_1(image, label, model, clip_classifier, alpha=alpha)\n",
        "    loss_1_diffs.append(sample['loss_diff'])\n",
        "    sample_2 = function_2(image, label, model, clip_classifier, alpha=alpha)\n",
        "    loss_2_diffs.append(sample_2['loss_diff'])\n",
        "\n",
        "print(alphas)\n",
        "print(loss_1_diffs)\n",
        "print(loss_2_diffs)"
      ],
      "metadata": {
        "id": "S5u_eNkliGS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations non-feature"
      ],
      "metadata": {
        "id": "OHZkdLvDpL9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math"
      ],
      "metadata": {
        "id": "B7rEUb7kpv7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap of adversarial perturbations"
      ],
      "metadata": {
        "id": "7hpm2zLUdhc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def comparison_grid(images, h_batch, h_adv_batch, num_rows=10):\n",
        "    \"\"\"\n",
        "    Create a grid of size `num_rows × 4`:\n",
        "      col1: image\n",
        "      col2: ‖h‖ heatmap\n",
        "      col3: ‖h_adv‖ heatmap\n",
        "      col4: ‖h_adv - h‖ heatmap\n",
        "\n",
        "    Arguments:\n",
        "        images:      tensor (B,3,H,W)  batch of input images\n",
        "        h_batch:     tensor (B,50,dim) clean tokens\n",
        "        h_adv_batch: tensor (B,50,dim) adversarial tokens\n",
        "        num_rows:    number of images to include (default=10)\n",
        "\n",
        "    Returns:\n",
        "        fig, axes\n",
        "    \"\"\"\n",
        "    B = images.shape[0]\n",
        "    assert B >= num_rows, f\"Batch only has {B} images but num_rows={num_rows}\"\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(num_rows, 4, figsize=(20, 4 * num_rows))\n",
        "\n",
        "    if num_rows == 1:\n",
        "        axes = axes.reshape(1, 4)\n",
        "\n",
        "    for row in range(num_rows):\n",
        "        # image\n",
        "        img = unnormalize(images[row]).squeeze().cpu().numpy()  # (3,H,W)\n",
        "        img = np.transpose(img, (1,2,0))              # (H,W,3)\n",
        "\n",
        "        # clean and adv tokens\n",
        "        h = h_batch[row].cpu().numpy()       # (50,dim)\n",
        "        h_adv = h_adv_batch[row].cpu().numpy()\n",
        "\n",
        "        # remove CLS\n",
        "        h_img = h[1:]         # (49, dim)\n",
        "        h_adv_img = h_adv[1:]\n",
        "\n",
        "        # compute norms\n",
        "        h_norm = np.linalg.norm(h_img, axis=-1)\n",
        "        h_adv_norm = np.linalg.norm(h_adv_img, axis=-1)\n",
        "        diff = np.linalg.norm(h_adv_img - h_img, axis=-1)\n",
        "\n",
        "        # reshape into 7×7 patch grid\n",
        "        h_map = h_norm.reshape(7,7)\n",
        "        h_adv_map = h_adv_norm.reshape(7,7)\n",
        "        diff_map = diff.reshape(7,7)\n",
        "\n",
        "        # upscale to image-size for visualization\n",
        "        def upscale(m):\n",
        "            return cv2.resize(m, (224,224), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        h_big = upscale(h_map)\n",
        "        h_adv_big = upscale(h_adv_map)\n",
        "        diff_big = upscale(diff_map)\n",
        "\n",
        "        axes[row,0].imshow(img)\n",
        "        axes[row,0].set_title(f\"Image {row}\")\n",
        "        axes[row,0].axis(\"off\")\n",
        "\n",
        "        # clean heatmap\n",
        "        im1 = axes[row,1].imshow(h_big, cmap=\"viridis\")\n",
        "        axes[row,1].set_title(\"‖h‖\")\n",
        "        axes[row,1].axis(\"off\")\n",
        "        fig.colorbar(im1, ax=axes[row,1], fraction=0.046, pad=0.04)\n",
        "\n",
        "        # adv heatmap\n",
        "        im2 = axes[row,2].imshow(h_adv_big, cmap=\"viridis\")\n",
        "        axes[row,2].set_title(\"‖h_adv‖\")\n",
        "        axes[row,2].axis(\"off\")\n",
        "        fig.colorbar(im2, ax=axes[row,2], fraction=0.046, pad=0.04)\n",
        "\n",
        "        # diff heatmap\n",
        "        im3 = axes[row,3].imshow(diff_big, cmap=\"viridis\")\n",
        "        axes[row,3].set_title(\"‖h_adv − h‖\")\n",
        "        axes[row,3].axis(\"off\")\n",
        "        fig.colorbar(im3, ax=axes[row,3], fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # return fig, axes"
      ],
      "metadata": {
        "id": "e2YakVhzZCom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for k, v in eval_dataset.items():\n",
        "k, v = list(eval_dataset.items())[4]\n",
        "graph_images = torch.stack(v, dim=0).to(device)\n",
        "k_list = torch.full((len(v),), k, device=device)\n",
        "sample = function_1(graph_images, k_list, model, clip_classifier)\n",
        "sample_2 = function_2(graph_images, k_list, model, clip_classifier)\n",
        "\n",
        "graph_h = sample['h'].detach().cpu().squeeze(0)\n",
        "graph_h_adv = sample['h_adv'].detach().cpu().squeeze(0)\n",
        "\n",
        "graph_x_adv = sample_2['x_adv'].detach().cpu().squeeze(0)\n",
        "graph_hx_adv = sample_2['hx_adv'].detach().cpu().squeeze(0)\n",
        "\n",
        "comparison_grid(graph_images, graph_h, graph_h_adv)\n",
        "comparison_grid(graph_x_adv, graph_h, graph_hx_adv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "PgRdCGzzbEqF",
        "outputId": "da705bee-498e-4f82-ee4b-1873dde529e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1063140166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for k, v in eval_dataset.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgraph_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_norm_difference(h, h_adv):\n",
        "    # h and h_adv have shape (50, 768)\n",
        "\n",
        "    # 1. Remove CLS token (token 0)\n",
        "    h_img = h[1:]              # (49, 768)\n",
        "    h_adv_img = h_adv[1:]      # (49, 768)\n",
        "\n",
        "    # 2. Compute per-token norms\n",
        "    h_norms = torch.norm(h_img, dim=-1)          # (49,)\n",
        "    h_adv_norms = torch.norm(h_adv_img, dim=-1)  # (49,)\n",
        "\n",
        "    # 3. Average norms across all spatial tokens\n",
        "    avg_h = h_norms.mean()\n",
        "    avg_h_adv = h_adv_norms.mean()\n",
        "\n",
        "    # 4. Average difference norm\n",
        "    avg_diff = torch.norm(h_adv_img - h_img, dim=-1).mean()\n",
        "    return avg_diff\n"
      ],
      "metadata": {
        "id": "UDS6HUnidr5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(avg_norm_difference(graph_h, graph_h_adv))\n",
        "print(avg_norm_difference(graph_h, graph_hx_adv))"
      ],
      "metadata": {
        "id": "h0Z_tLVOeB5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature visualization of perturbations"
      ],
      "metadata": {
        "id": "cDGH1fmpqvlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap 1d: x, y = position of activation, color = how strongly a feature fires per patch (for h, h_adv)\n",
        "\n",
        "import\n",
        "def feature_visualization(h, features, k=5, cols=3):\n",
        "    \"\"\"\n",
        "    Visualize CLIP patch activations (2D) and top-k SAE features (2D)\n",
        "    in a grid layout with `cols` images per row.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    h : tensor (50, 768)\n",
        "        CLIP hidden activations. Token 0 = CLS, 1–49 = 7×7 patches.\n",
        "    features : tensor (50, F)\n",
        "        SAE encoder output features for each token.\n",
        "    k : int\n",
        "        Number of top SAE features to visualize.\n",
        "    cols : int\n",
        "        Number of heatmaps per row (default: 3).\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare patch-level norm for h\n",
        "    h_img = h[1:]                                 # (49, 768)\n",
        "    h_norm = torch.norm(h_img, dim=-1).cpu()      # (49,)\n",
        "    h_map = h_norm.reshape(7, 7).numpy()          # (7, 7)\n",
        "\n",
        "    # Rank SAE features (top-k)\n",
        "    feat_img = features[1:]                       # (49, F)\n",
        "    feature_scores = feat_img.abs().sum(dim=0)    # (F,)\n",
        "    topk_scores, topk_idx = torch.topk(feature_scores, k)\n",
        "\n",
        "    # Total number of images: 1 (for h) + k (features)\n",
        "    total = 1 + k\n",
        "    rows = math.ceil(total / cols)\n",
        "\n",
        "    # Create large grid figure\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
        "    axes = axes.flatten()  # always flatten for easy indexing\n",
        "\n",
        "    # First heatmap = h patch magnitude\n",
        "    ax = axes[0]\n",
        "    hm = ax.imshow(h_map, cmap=\"viridis\")\n",
        "    ax.set_title(\"CLIP h — Patch Magnitude\", fontsize=12)\n",
        "    ax.axis(\"off\")\n",
        "    fig.colorbar(hm, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # plot the top-k features in 2D\n",
        "    for i in range(k):\n",
        "        feat_id = topk_idx[i].item()\n",
        "        score = topk_scores[i].item()\n",
        "\n",
        "        activation = feat_img[:, feat_id].cpu().numpy()  # (49,)\n",
        "        activation_2d = activation.reshape(7, 7)          # (7, 7)\n",
        "\n",
        "        ax = axes[i+1]\n",
        "        hm = ax.imshow(activation_2d, cmap=\"magma\")\n",
        "        ax.set_title(\n",
        "            f\"Top Feature #{i+1}\\nID {feat_id} | Score {score:.2f}\",\n",
        "            fontsize=11\n",
        "        )\n",
        "        ax.axis(\"off\")\n",
        "        fig.colorbar(hm, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Hide unused axes\n",
        "    for j in range(total, len(axes)):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # return fig"
      ],
      "metadata": {
        "id": "ZgA91zOhrFI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "LRHupe-j492J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis dataset"
      ],
      "metadata": {
        "id": "ee2w3cYxVgYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "structure:\n",
        "dataset = [\n",
        "  {\n",
        "    \"image\"\n",
        "    \"label\"\n",
        "    \"meta\"\n",
        "  },\n",
        "...\n",
        "]\n",
        "'''\n",
        "\n",
        "def get_images_per_class(dataset, num_per_class=5):\n",
        "    counts = {label: 0 for label in range(10)}\n",
        "    items = []  # list of dicts\n",
        "\n",
        "    for idx in range(len(dataset)):\n",
        "        img, label = dataset[idx]\n",
        "\n",
        "        if counts[label] < num_per_class:\n",
        "            item = {\n",
        "                \"image\": img,\n",
        "                \"label\": label,\n",
        "                \"meta\": {}\n",
        "            }\n",
        "            items.append(item)\n",
        "            counts[label] += 1\n",
        "\n",
        "        # stop when all classes collected\n",
        "        if all(counts[l] >= num_per_class for l in counts):\n",
        "            break\n",
        "\n",
        "    return items\n"
      ],
      "metadata": {
        "id": "VYpt-dtdVgIR"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_per_class = 5\n",
        "eval_dataset = get_images_per_class(test_dataset, num_per_class)"
      ],
      "metadata": {
        "id": "lCsZVyZ1kVMm"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [1064, 2420, 2642, 5167, 6847, 7636, 8709, 9028, 10216, 13978, 16979,19030, 20359, 21971, 24248, 25461, 25989, 29216, 29390, 31041, 40471,43948, 44551, 47241]\n",
        "\n",
        "feature_to_index = {f: i for i, f in enumerate(features)}"
      ],
      "metadata": {
        "id": "RfyE1MjOkNb6"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_outputs(x, y, model, clip_classifier, feature_list):\n",
        "  results = get_all_outputs(x, y, model, clip_classifier)\n",
        "  results[\"h_features\"] = results[\"h_features\"][:, :, feature_list]\n",
        "  results[\"h_adv_features\"] = results[\"h_adv_features\"][:,:, feature_list]\n",
        "  results[\"hx_adv_features\"] = results[\"hx_adv_features\"][:,:, feature_list]\n",
        "  return results"
      ],
      "metadata": {
        "id": "lW34lP-fkE6h"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for each image in the dataset:\n",
        "  - pre-adversarial: x, h, SAE(h)\n",
        "  - adversarial (on inputs): x_adv, h(x_adv), SAE(h(x_adv)) (shape 1, 50, 49k)\n",
        "  - adversarial (on hidden state): h, h_adv, SAE(h_adv)\n",
        "'''\n",
        "\n",
        "for n in eval_dataset:\n",
        "    n['meta'] = get_feature_outputs(n['image'].to(device).unsqueeze(0), torch.tensor(n['label']).unsqueeze(0).to(device),\n",
        "                                model, clip_classifier, features)"
      ],
      "metadata": {
        "id": "wtJJ1KNxkJU4"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = sorted(eval_dataset, key=lambda x: x[\"label\"])"
      ],
      "metadata": {
        "id": "j_B5NA2OkaIy"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(eval_dataset, f\"eval_dataset_{num_per_class}.pt\")"
      ],
      "metadata": {
        "id": "aslIPa0xZmJd"
      },
      "execution_count": 153,
      "outputs": []
    }
  ]
}